{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "congressional-atlantic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Dig)\n",
      "0000\n",
      "\u001b[41m0\u001b[0m000\n",
      "0000\n",
      "0000\n",
      "\n",
      "episode: 4\n",
      "step: 30\n",
      "action: 4\n",
      "reward: 1\n",
      "total_reward: 8\n",
      "done: True\n",
      "battery: 970\n",
      "score: 19\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import gym_digger\n",
    "from os import system\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "IN_FILE = 'saved_weights/dqn_Digger-v0_4x4_h_weights_a.h5f'\n",
    "\n",
    "# environment settings\n",
    "ENV_NAME = 'Digger-v0'\n",
    "MAP_NAME = '4x4_h'\n",
    "BATTERY = 1000\n",
    "COMPLETION_BONUS = 0\n",
    "BATTERY_PENALTY = 0\n",
    "\n",
    "# keras settings\n",
    "SEQUENTIAL_MEMORY = 100000\n",
    "WINDOW_LENGTH = 1\n",
    "STEPS_WARMUP = 10\n",
    "TARGET_MODEL_UPDATE = 1e-2\n",
    "LEARNING_RATE = 1e-3\n",
    "STEPS = 400000\n",
    "\n",
    "# build the environment\n",
    "env = gym.make(ENV_NAME, map_name=MAP_NAME, battery=BATTERY, completion_bonus=COMPLETION_BONUS,\n",
    "               battery_penalty=BATTERY_PENALTY)\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "# build the model\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "# configure and compile the agent\n",
    "memory = SequentialMemory(limit=SEQUENTIAL_MEMORY, window_length=WINDOW_LENGTH)\n",
    "policy = BoltzmannQPolicy()\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=STEPS_WARMUP,\n",
    "               target_model_update=TARGET_MODEL_UPDATE, policy=policy)\n",
    "dqn.compile(Adam(lr=LEARNING_RATE), metrics=['mae'])\n",
    "\n",
    "# load the weights\n",
    "dqn.load_weights(IN_FILE)\n",
    "\n",
    "# run the test\n",
    "for episode in range(5):\n",
    "    observation = env.reset()\n",
    "    print(f'Start episode {episode}', end='')\n",
    "    env.render()\n",
    "    time.sleep(2)\n",
    "    clear_output(wait=True)\n",
    "    done = False\n",
    "    step = 0\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        step += 1\n",
    "        action = dqn.forward(observation)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        battery = info['battery']\n",
    "        score = info['score']\n",
    "        total_reward += reward\n",
    "\n",
    "        # render it\n",
    "        env.render()       \n",
    "        \n",
    "        # print stats\n",
    "        print(f'\\nepisode: {episode}\\nstep: {step}\\naction: {action}\\nreward: {reward}\\ntotal_reward: {total_reward}\\ndone: {done}\\nbattery: {battery}\\nscore: {score}')\n",
    "\n",
    "        # delay\n",
    "        time.sleep(0.5)\n",
    "        clear_output(wait=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "gym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
